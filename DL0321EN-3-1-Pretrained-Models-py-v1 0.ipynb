{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item31'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import skillsnetwork "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d0639b47984f1f9296365336992f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading concrete_data_week3.zip:   0%|          | 0/97863179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6975d2b103cc42bf885ebe9725bc9afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "## get the data\n",
    "await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/concrete_data_week3.zip\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ImageDataGenerator Instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in this lab, we will be using the full data-set of 30,000 images for training and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',  # Validation data directory\n",
    "    target_size=(image_resize, image_resize),  # Resize images to 224x224\n",
    "    batch_size=batch_size_validation,  # Batch size for validation\n",
    "    class_mode='categorical'  # Since we have two classes (positive, negative)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Compile and Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1801: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3661: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 16:39:54.912637: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2025-03-06 16:39:54.917927: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394310000 Hz\n",
      "2025-03-06 16:39:54.918450: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55efea4cd290 executing computations on platform Host. Devices:\n",
      "2025-03-06 16:39:54.918491: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2025-03-06 16:39:54.949360: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3665: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7f72585ece50>,\n",
       " <keras.layers.core.Dense at 0x7f724275dd90>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the ResNet50 layers by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7f72d19cb090>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f72c9c7b190>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72c9c7ba90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72c9c7bdd0>,\n",
       " <keras.layers.core.Activation at 0x7f72cbce4910>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f72cbcf5790>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72cb292150>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72cb1a56d0>,\n",
       " <keras.layers.core.Activation at 0x7f72cb1a5e10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72cb1b9a10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72cb2d5b10>,\n",
       " <keras.layers.core.Activation at 0x7f72cb120fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72c8433850>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72c8334810>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72c8399f10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72c82ae250>,\n",
       " <keras.layers.merge.Add at 0x7f72c82ebdd0>,\n",
       " <keras.layers.core.Activation at 0x7f72c823ae50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72c8182a50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72c81b3dd0>,\n",
       " <keras.layers.core.Activation at 0x7f72c81b3ad0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72c80f0dd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72c8045410>,\n",
       " <keras.layers.core.Activation at 0x7f72c8045990>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72b879c850>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72b8715f50>,\n",
       " <keras.layers.merge.Add at 0x7f72b8731250>,\n",
       " <keras.layers.core.Activation at 0x7f72b86b5cd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72b86679d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72b862fa90>,\n",
       " <keras.layers.core.Activation at 0x7f72b85f5f90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72b85b2ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72b84d31d0>,\n",
       " <keras.layers.core.Activation at 0x7f72b852c410>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72b844a5d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72b83c1f10>,\n",
       " <keras.layers.merge.Add at 0x7f72b83de210>,\n",
       " <keras.layers.core.Activation at 0x7f72b8362f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72b8314950>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72b82dcad0>,\n",
       " <keras.layers.core.Activation at 0x7f72b82c5210>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72b825be50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72b81d71d0>,\n",
       " <keras.layers.core.Activation at 0x7f72b81d7290>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72b814c610>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72b8067ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72b8093e90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72947c2750>,\n",
       " <keras.layers.merge.Add at 0x7f72947c2990>,\n",
       " <keras.layers.core.Activation at 0x7f7294720310>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7294642a50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f729469ff50>,\n",
       " <keras.layers.core.Activation at 0x7f729469fd10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72945bed50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72945a2710>,\n",
       " <keras.layers.core.Activation at 0x7f72945a2f10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72944baad0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72944b4150>,\n",
       " <keras.layers.merge.Add at 0x7f72944b4210>,\n",
       " <keras.layers.core.Activation at 0x7f72943d2150>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72943f4c50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72943b65d0>,\n",
       " <keras.layers.core.Activation at 0x7f729434dd50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72942ebd10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7294278210>,\n",
       " <keras.layers.core.Activation at 0x7f7294252650>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72941e8e90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7294162d50>,\n",
       " <keras.layers.merge.Add at 0x7f72941620d0>,\n",
       " <keras.layers.core.Activation at 0x7f729407f1d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f727c7d0150>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7294078b10>,\n",
       " <keras.layers.core.Activation at 0x7f7294078f90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f727c75ad50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f727c6e3190>,\n",
       " <keras.layers.core.Activation at 0x7f729407f450>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f727c652690>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f727c59aa90>,\n",
       " <keras.layers.merge.Add at 0x7f727c5d1110>,\n",
       " <keras.layers.core.Activation at 0x7f727c56a650>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f727c56a990>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f727c4ceb10>,\n",
       " <keras.layers.core.Activation at 0x7f727c405190>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f727c405fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f727c3a0410>,\n",
       " <keras.layers.core.Activation at 0x7f727c3e5110>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f727c2fced0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f727c217210>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f727c2e9a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f727c237ad0>,\n",
       " <keras.layers.merge.Add at 0x7f727c1ccb50>,\n",
       " <keras.layers.core.Activation at 0x7f727c11ee10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f727c0e6a10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f727c099d10>,\n",
       " <keras.layers.core.Activation at 0x7f727c099590>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7259f8a450>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7259f6de90>,\n",
       " <keras.layers.core.Activation at 0x7f7259f6db50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7259e8dfd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7259e053d0>,\n",
       " <keras.layers.merge.Add at 0x7f7259e056d0>,\n",
       " <keras.layers.core.Activation at 0x7f7259d9ea10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7259d9ecd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7259d1b210>,\n",
       " <keras.layers.core.Activation at 0x7f7259d1b150>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7259cb7450>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7259c31d50>,\n",
       " <keras.layers.core.Activation at 0x7f7259c310d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7259b4e3d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7259b30c10>,\n",
       " <keras.layers.merge.Add at 0x7f7259b30550>,\n",
       " <keras.layers.core.Activation at 0x7f7259a4f710>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7259a177d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72599c7f90>,\n",
       " <keras.layers.core.Activation at 0x7f72599c70d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72598fed50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72598e3910>,\n",
       " <keras.layers.core.Activation at 0x7f72598e3e90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7259877f50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72597f1110>,\n",
       " <keras.layers.merge.Add at 0x7f72597f11d0>,\n",
       " <keras.layers.core.Activation at 0x7f7259712fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72598fe490>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72596f3a90>,\n",
       " <keras.layers.core.Activation at 0x7f7259688950>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7259626b50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f725958fed0>,\n",
       " <keras.layers.core.Activation at 0x7f725958fd50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72594bdd50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72594a6990>,\n",
       " <keras.layers.merge.Add at 0x7f72594a6fd0>,\n",
       " <keras.layers.core.Activation at 0x7f72593bdad0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72593e0890>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72593b6c50>,\n",
       " <keras.layers.core.Activation at 0x7f72593b65d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72592eea90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7259275dd0>,\n",
       " <keras.layers.core.Activation at 0x7f725924b490>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72591e9d50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7259152f90>,\n",
       " <keras.layers.merge.Add at 0x7f7259152e10>,\n",
       " <keras.layers.core.Activation at 0x7f7259083d50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f72591e9ad0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7259063f10>,\n",
       " <keras.layers.core.Activation at 0x7f7258fffd50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7258f98750>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7258f7af90>,\n",
       " <keras.layers.core.Activation at 0x7f7258e9c990>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7258eb0f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7258dac9d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7258e13610>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7258d630d0>,\n",
       " <keras.layers.merge.Add at 0x7f7258d63e50>,\n",
       " <keras.layers.core.Activation at 0x7f7258c6bd90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7258c41a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7258bbef90>,\n",
       " <keras.layers.core.Activation at 0x7f7258bbed90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7258b5e8d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7258ac8a10>,\n",
       " <keras.layers.core.Activation at 0x7f7258ac8e10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7258a74250>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72589d6a90>,\n",
       " <keras.layers.merge.Add at 0x7f72589d63d0>,\n",
       " <keras.layers.core.Activation at 0x7f7258974c50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f725892b410>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f72588ef350>,\n",
       " <keras.layers.core.Activation at 0x7f72588ef450>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7258828f90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f725873e390>,\n",
       " <keras.layers.core.Activation at 0x7f7258787f90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f725871fe90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f725868aa50>,\n",
       " <keras.layers.merge.Add at 0x7f725868ae50>,\n",
       " <keras.layers.core.Activation at 0x7f7258639290>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7f7258974590>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x7f72c9c7bfd0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile our model using the **adam** optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/1\n",
      "101/101 [==============================] - 10291s 102s/step - loss: 0.0836 - acc: 0.9749 - val_loss: 0.4959 - val_acc: 0.7662\n"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "prev_pub_hash": "cf2970a1d2c549fe86023eaa076d0ce4936c4275baf2cccfdad8fe6ce3a8a6c2"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
